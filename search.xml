<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2021/01/21/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a class="link"   href="https://hexo.io/" >Hexo<i class="fas fa-external-link-alt"></i></a>! This is your very first post. Check <a class="link"   href="https://hexo.io/docs/" >documentation<i class="fas fa-external-link-alt"></i></a> for more info. If you get any problems when using Hexo, you can find the answer in <a class="link"   href="https://hexo.io/docs/troubleshooting.html" >troubleshooting<i class="fas fa-external-link-alt"></i></a> or you can ask me on <a class="link"   href="https://github.com/hexojs/hexo/issues" >GitHub<i class="fas fa-external-link-alt"></i></a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a class="link"   href="https://hexo.io/docs/writing.html" >Writing<i class="fas fa-external-link-alt"></i></a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a class="link"   href="https://hexo.io/docs/server.html" >Server<i class="fas fa-external-link-alt"></i></a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a class="link"   href="https://hexo.io/docs/generating.html" >Generating<i class="fas fa-external-link-alt"></i></a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a class="link"   href="https://hexo.io/docs/one-command-deployment.html" >Deployment<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
  </entry>
  <entry>
    <title>把Hexo部署到Github Page</title>
    <url>/2021/01/21/%E6%8A%8AHexo%E9%83%A8%E7%BD%B2%E5%88%B0Github-Page/</url>
    <content><![CDATA[<h1 id="使用Hexo搭建博客系统部署到Github"><a href="#使用Hexo搭建博客系统部署到Github" class="headerlink" title="使用Hexo搭建博客系统部署到Github"></a>使用Hexo搭建博客系统部署到Github</h1><h3 id="1-Hexo"><a href="#1-Hexo" class="headerlink" title="1.Hexo"></a>1.Hexo</h3><p>官网地址：<a class="link"   href="https://hexo.io/zh-cn/" >https://hexo.io/zh-cn/<i class="fas fa-external-link-alt"></i></a></p>
<p>快速、简洁且高效的博客框架</p>
<h3 id="2-安装Git，Node-js"><a href="#2-安装Git，Node-js" class="headerlink" title="2.安装Git，Node.js"></a>2.安装Git，Node.js</h3><h3 id="3-安装Hexo"><a href="#3-安装Hexo" class="headerlink" title="3.安装Hexo"></a>3.安装Hexo</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo init &lt;folder&gt;</span><br><span class="line">cd &lt;folder&gt;</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>
<h3 id="4-修改配置-config-yml"><a href="#4-修改配置-config-yml" class="headerlink" title="4.修改配置 _config.yml"></a>4.修改配置 <code>_config.yml</code></h3><p>官网有详细说明，这里我就不列举了</p>
<h3 id="5-本地启动服务"><a href="#5-本地启动服务" class="headerlink" title="5.本地启动服务"></a>5.本地启动服务</h3><p><code>hexo g</code>  生成静态文件</p>
<p><code>hexo s </code> 启动服务，默认端口： 4000</p>
<h3 id="6-选取主题"><a href="#6-选取主题" class="headerlink" title="6.选取主题"></a>6.选取主题</h3><p>官网主题地址：<a class="link"   href="https://hexo.io/themes/" >https://hexo.io/themes/<i class="fas fa-external-link-alt"></i></a></p>
<p>进到主题内有详细的git仓库地址，有安装说明，基本就是把git仓库下载到本地 <code>themes/</code> 目录下。</p>
<p>修改_config.yml里的theme为新建主题目录的名字</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>之后刷新页面看下效果</p>
<h3 id="7-添加文章"><a href="#7-添加文章" class="headerlink" title="7.添加文章"></a>7.添加文章</h3><p>hexo new “your-post-name”</p>
<p>如果想在本地存放图片等信息需要在_config.yml修改</p>
<p>post_asset_folder: true</p>
<p>开启这个配置当新建文件时会在本地创建两个文件，一个是存Markdown文档的，一个储存图片和音频的</p>
<h3 id="8-部署到Github"><a href="#8-部署到Github" class="headerlink" title="8.部署到Github"></a>8.部署到Github</h3><p>首先到Github创建一个公开的项目，名字为你的<code>账号名.github.io</code> </p>
<p>此处一定要设置为账号名为前缀的域名，不然后续会有一些坑，导致你的页面显示不了JS</p>
<h3 id="9-在设置中配置-Github-Pages-选择分支"><a href="#9-在设置中配置-Github-Pages-选择分支" class="headerlink" title="9.在设置中配置 Github Pages 选择分支"></a>9.在设置中配置 Github Pages 选择分支</h3><h3 id="10-在本地配置链接Github"><a href="#10-在本地配置链接Github" class="headerlink" title="10.在本地配置链接Github"></a>10.在本地配置链接Github</h3><p>安装  <code>npm install hexo-deployer-git --save</code></p>
<p>配置_config.yml</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: 你的git地址</span><br><span class="line">  branch: 分支</span><br></pre></td></tr></table></figure>
<p>hexo g</p>
<p>hexo d</p>
<p>出现INFO Deploy done: git则大功告成，可以去GitHub上访问看看在线网站</p>
<p>参考文章地址：<a class="link"   href="https://www.jianshu.com/p/282717c8da6c" >https://www.jianshu.com/p/282717c8da6c<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
  </entry>
  <entry>
    <title>Flink特性</title>
    <url>/2021/01/21/Flink%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<h1 id="Flink特性"><a href="#Flink特性" class="headerlink" title="Flink特性"></a>Flink特性</h1><p>一、统一的数据处理组件栈，不仅能处理流处理，还可以用于批处理，机器学习，可以满足不同的用户的需求，对不同形式的计算进行了整合。</p>
<p>二、支持时间时间，接入时间，处理时间等时间概念。</p>
<p>三、基于轻量级分布式快照实现容错（checkpoint）。</p>
<p>四、有状态计算。可以将状态存储在HDFS，内存，RockDB。</p>
<p>五、支持高度灵活的窗口，比如：session window，滚动窗口，滑动窗口等。</p>
<p>六、反压，当下流算子处理不过来的时候，对上层算子的消费速度进行控制。</p>
<p>七、基于JVM实现自己的内存管理。</p>
]]></content>
  </entry>
  <entry>
    <title>定期删除Hive表的过期数据</title>
    <url>/2021/01/23/%E5%AE%9A%E6%9C%9F%E5%88%A0%E9%99%A4Hive%E8%A1%A8%E7%9A%84%E8%BF%87%E6%9C%9F%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p>由于Hive中有很多表都是每日全量的，数据量比较大，有些是可以将过去历史分区的数据进行删除的，所以需要一个定时执行的脚本，定时删除前七天的过期数据。</p>
<p>注：此脚本只针对分区字段为日期类型</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">/bin/bash</span></span><br><span class="line"></span><br><span class="line">today=$(date +%Y-%m-%d)</span><br><span class="line">today_timestamp=$(date -d &quot;$today&quot; +%s)</span><br><span class="line">echo &quot;获取今天日期：$today,今日日期的时间戳：$today_timestamp&quot;</span><br><span class="line">detele_day=$(date -d &quot;7 days ago&quot; +%Y-%m-%d)</span><br><span class="line">detele_day_timestamp=$(date -d &quot;$detele_day&quot; +%s)</span><br><span class="line">echo &quot;获取60天前日期：$detele_day,60天前日期的时间戳：$detele_day_timestamp&quot;</span><br><span class="line">hdfs_directorys=(</span><br><span class="line">  /apps/hive/warehouse/某个库/某个表/</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">for hdfs_directory in $&#123;hdfs_directorys[@]&#125;; do</span><br><span class="line">  echo &quot;当前要删除的hdfs路径为：$&#123;hdfs_directory&#125;&quot;</span><br><span class="line">  for hdfs_directory_p_day in $(sudo -u hdfs hdfs dfs -ls $&#123;hdfs_directory&#125; | awk &#x27;BEGIN&#123;FS=&quot; &quot;&#125;&#123;print $8&#125;&#x27;); do</span><br><span class="line">    p_day=$(echo $hdfs_directory_p_day | awk &#x27;BEGIN&#123;FS=&quot;=&quot;&#125;&#123;print $2&#125;&#x27;)</span><br><span class="line">    p_day_timestamp=$(date -d &quot;$p_day&quot; +%s)</span><br><span class="line">    echo &quot;当前文件夹分区日期为：$p_day, 当前文件夹分区日期时间戳为：$p_day_timestamp&quot;</span><br><span class="line">    if [ $p_day_timestamp -lt $detele_day_timestamp ]; then</span><br><span class="line">      echo &quot;该HDFS的目录即将被删除，路径为：$hdfs_directory_p_day&quot;</span><br><span class="line">      sudo -u hdfs hadoop dfs -rm -r -skipTrash $hdfs_directory_p_day</span><br><span class="line">    else</span><br><span class="line">      echo &quot;该HDFS的目录处于正常周期内不会被删除，路径为：$hdfs_directory_p_day&quot;</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br><span class="line">echo &quot;程序运行完成&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>SparkStreaming延迟监控</title>
    <url>/2021/01/23/SparkStreaming%E5%BB%B6%E8%BF%9F%E7%9B%91%E6%8E%A7/</url>
    <content><![CDATA[<p>​        这篇博客来源于一个惨痛的线上事故经历，我们编写好SparkStreaming程序清洗行为数据，然后每十分钟往Hive写一次，大家都以为任务正常的运行，不会出什么问题，然而问题正在后台默默的产生了，到了第二天，所有依赖于Hive这张行为数据表的报表数据都少了很多，这是为啥呢？为什么会有这个问题？答案：数据过多，Spark Streaming调度批次积压，再加上数据倾斜，导致一个批次任务运行时间超过了原来正常运行时间的二倍，数据延迟三个小时。</p>
<p>​        这种事故最快的解决办法就是把报表任务再跑一遍，数据就全了，但是治标不治本，必须根据数据延迟情况适当调整资源和Kafka的Topic分区数，怎么才能知道Spark Streaming任务什么时候延迟呢？以及延迟情况是怎么样的呢？大家可能都知道去Spark Streaming的Web UI去看呀，地址：<a class="link"   href="http://resourcemanager地址/proxy/application_id/streaming/%EF%BC%8C%E9%97%AE%E9%A2%98%E6%9D%A5%E4%BA%86%EF%BC%8C%E6%88%91%E4%BB%AC%E8%83%BD%E5%A4%A9%E5%A4%A9%E7%9B%AF%E7%9D%80%E5%AE%83%E7%9C%8B%E5%90%97%EF%BC%8C%E8%80%8C%E4%B8%94%E6%9C%89%E5%8F%AF%E8%83%BD%E6%97%B6%E9%97%B4%E4%B8%8D%E5%9B%BA%E5%AE%9A%EF%BC%8C%E5%90%8E%E6%9D%A5%E7%94%A8%E4%BA%86%E7%88%AC%E8%99%AB%E7%9A%84%E6%80%9D%E6%83%B3%EF%BC%8C%E5%85%88%E8%AF%B7%E6%B1%82%E4%B8%80%E4%B8%8B%E7%9B%91%E6%8E%A7%E7%95%8C%E9%9D%A2%EF%BC%8C%E7%9C%8B%E7%9C%8B%E8%83%BD%E6%8B%BF%E5%88%B0%E5%93%AA%E4%BA%9B%E4%BF%A1%E6%81%AF%EF%BC%8C%E7%84%B6%E5%90%8E%E5%9C%A8%E6%B8%85%E6%B4%97%E4%B8%80%E4%B8%8B%EF%BC%8C%E6%8A%8A%E5%85%B3%E9%94%AE%E7%9A%84%E6%8C%87%E6%A0%87%E6%8B%BF%E5%87%BA%E6%9D%A5%EF%BC%8C%E8%B6%85%E8%BF%87%E9%98%88%E5%80%BC%E5%88%99%E6%8A%A5%E8%AD%A6%EF%BC%8C%E8%BF%99%E6%A0%B7%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%BF%AB%E9%80%9F%E7%9F%A5%E9%81%93%E7%A7%AF%E5%8E%8B%E6%83%85%E5%86%B5%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%8F%8A%E6%97%B6%E5%A4%84%E7%90%86%EF%BC%8C%E4%BB%A5%E9%98%B2%E4%BA%8B%E6%95%85%E5%86%8D%E6%AC%A1%E5%8F%91%E7%94%9F%E3%80%82" >http://resourcemanager地址/proxy/application_id/streaming/，问题来了，我们能天天盯着它看吗，而且有可能时间不固定，后来用了爬虫的思想，先请求一下监控界面，看看能拿到哪些信息，然后在清洗一下，把关键的指标拿出来，超过阈值则报警，这样我们可以快速知道积压情况，并且及时处理，以防事故再次发生。<i class="fas fa-external-link-alt"></i></a></p>
<p>​        参数说明:–application_name : Spark Streaming代码里的application_name，–active_batches：最高可接收延迟的批次数，大于此值则报警。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">from</span> optparse <span class="keyword">import</span> OptionParser</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line">sys.path.append(BASE_DIR)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">option_parser</span>():</span></span><br><span class="line">    usage = <span class="string">&quot;usage: %prog [options] arg1&quot;</span></span><br><span class="line"></span><br><span class="line">    parser = OptionParser(usage=usage)</span><br><span class="line"></span><br><span class="line">    parser.add_option(<span class="string">&quot;--application_name&quot;</span>, dest=<span class="string">&quot;application_name&quot;</span>, action=<span class="string">&quot;store&quot;</span>, <span class="built_in">type</span>=<span class="string">&quot;string&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    parser.add_option(<span class="string">&quot;--active_batches&quot;</span>, dest=<span class="string">&quot;active_batches&quot;</span>, action=<span class="string">&quot;store&quot;</span>, <span class="built_in">type</span>=<span class="string">&quot;string&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parser</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    reload(sys)</span><br><span class="line">    sys.setdefaultencoding(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    optParser = option_parser()</span><br><span class="line">    options, args = optParser.parse_args(sys.argv[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (options.application_name <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> options.active_batches <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;请指定完整参数--application_name --active_batches&quot;</span></span><br><span class="line">        exit(<span class="number">1</span>)</span><br><span class="line">    active_batche = <span class="number">0</span></span><br><span class="line">    record = <span class="string">&quot;&quot;</span></span><br><span class="line">    schedul = <span class="string">&quot;&quot;</span></span><br><span class="line">    resourcemanager_url = <span class="string">&quot;resourcemanager_url&quot;</span> <span class="comment"># 例：http://localhost:8088/cluster/scheduler</span></span><br><span class="line">    resourcemanager_url_html = requests.get(resourcemanager_url).content.decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    html = etree.HTML(resourcemanager_url_html)</span><br><span class="line">    application_content = html.xpath(<span class="string">&#x27;//*[@id=&quot;apps&quot;]/script&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> application_content:</span><br><span class="line">        application_text_list = content.text.split(<span class="string">&quot;=&quot;</span>, <span class="number">1</span>)[<span class="number">1</span>].split(<span class="string">&quot;],&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> application_text <span class="keyword">in</span> application_text_list:</span><br><span class="line">            application_text = application_text.replace(<span class="string">&quot;[&quot;</span>, <span class="string">&quot;&quot;</span>).replace(<span class="string">&quot;]&quot;</span>, <span class="string">&quot;&quot;</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            application_name = application_text[<span class="number">2</span>].replace(<span class="string">&quot;\&quot;&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">            application_id = re.findall(<span class="string">&quot;&gt;(.*)&lt;&quot;</span>, <span class="built_in">str</span>(application_text[<span class="number">0</span>]))[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> (application_name == options.application_name):</span><br><span class="line">                streaming_url = <span class="string">&quot;http://localhost:8088/proxy/%s/streaming/&quot;</span> % application_id</span><br><span class="line">                streaming_html = requests.get(streaming_url).content.decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">                streaming_html = etree.HTML(streaming_html)</span><br><span class="line">                streaming_content_list = streaming_html.xpath(<span class="string">&#x27;//*[@id=&quot;active&quot;]&#x27;</span>)</span><br><span class="line">                <span class="comment"># 清洗active_batche</span></span><br><span class="line">                <span class="keyword">for</span> content <span class="keyword">in</span> streaming_content_list:</span><br><span class="line">                    active_batches = content.text</span><br><span class="line">                    active_batche = <span class="built_in">int</span>(re.findall(<span class="string">&quot;\((.*)\)&quot;</span>, active_batches)[<span class="number">0</span>])</span><br><span class="line">                streaming_records_list = streaming_html.xpath(<span class="string">&#x27;//*[@id=&quot;active-batches-table&quot;]/tbody/*/td[2]&#x27;</span>)</span><br><span class="line">                <span class="comment"># 清洗record</span></span><br><span class="line">                <span class="keyword">for</span> records <span class="keyword">in</span> streaming_records_list:</span><br><span class="line">                    record = records.text</span><br><span class="line">                streaming_scheduling_delay_list = streaming_html.xpath(<span class="string">&#x27;//*[@id=&quot;active-batches-table&quot;]/tbody/*/td[3]&#x27;</span>)</span><br><span class="line">                <span class="comment"># 清洗Scheduling Delay</span></span><br><span class="line">                <span class="keyword">for</span> scheduling <span class="keyword">in</span> streaming_scheduling_delay_list:</span><br><span class="line">                    schedul = scheduling.text</span><br><span class="line">                <span class="built_in">print</span> active_batche</span><br><span class="line">                <span class="keyword">if</span> (active_batche &gt; <span class="built_in">int</span>(options.active_batches)):</span><br><span class="line">                    content = <span class="string">&quot;任务%s延迟了,积压批次:%d,Records:%s,Scheduling Delay:%s&quot;</span> % (application_name, active_batche, record, schedul)</span><br><span class="line">                    <span class="built_in">print</span> content</span><br><span class="line">                    <span class="comment"># TODO 加上公司内部IM接口通知地址，时刻关注，推荐用飞书</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>Linux环境下安装FLink1.10.0并启动SQL-client读取Hive数据</title>
    <url>/2021/01/23/Linux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85FLink1-10-0%E5%B9%B6%E5%90%AF%E5%8A%A8SQL-client%E8%AF%BB%E5%8F%96Hive%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p><strong>Linux环境下安装FLink1.10.0并启动SQL-client读取Hive数据</strong></p>
<p>首先去官网下载Flink1.10.0的tgz的包，教程如上篇文章上半部分流程一样，然后配置一下<code>FLINK_HOME/conf/sql-client-defaults.yaml</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">catalogs:</span><br><span class="line">   - name: myhive   #自己定个名字就行</span><br><span class="line">     type: hive</span><br><span class="line">     hive-conf-dir: &#x2F;etc&#x2F;hive&#x2F;conf  # hive-site.xml的路径</span><br><span class="line">     hive-version: 1.2.1    # hive版本</span><br><span class="line"> execution:</span><br><span class="line">  # select the implementation responsible for planning table programs</span><br><span class="line">  # possible values are &#39;blink&#39; (used by default) or &#39;old&#39;</span><br><span class="line">  planner: blink</span><br><span class="line">  # &#39;batch&#39; or &#39;streaming&#39; execution</span><br><span class="line">  type: batch  #这里streaming和batch都行</span><br><span class="line">  # allow &#39;event-time&#39; or only &#39;processing-time&#39; in sources</span><br><span class="line">  time-characteristic: event-time</span><br><span class="line">  # interval in ms for emitting periodic watermarks</span><br><span class="line">  periodic-watermarks-interval: 200</span><br><span class="line">  # &#39;changelog&#39; or &#39;table&#39; presentation of results</span><br><span class="line">  result-mode: table</span><br><span class="line">  # maximum number of maintained rows in &#39;table&#39; presentation of results</span><br><span class="line">  max-table-result-rows: 1000000</span><br><span class="line">  # parallelism of the program</span><br><span class="line">  parallelism: 1</span><br><span class="line">  # maximum parallelism</span><br><span class="line">  max-parallelism: 128</span><br><span class="line">  # minimum idle state retention in ms</span><br><span class="line">  min-idle-state-retention: 0</span><br><span class="line">  # maximum idle state retention in ms</span><br><span class="line">  max-idle-state-retention: 0</span><br><span class="line">  # current catalog (&#39;default_catalog&#39; by default)</span><br><span class="line">  current-catalog: myhive</span><br><span class="line">  # current database of the current catalog (default database of the catalog by default)</span><br><span class="line">  current-database: secoo_tmp</span><br><span class="line">  # controls how table programs are restarted in case of a failures</span><br><span class="line">  restart-strategy:</span><br><span class="line">    # strategy type</span><br><span class="line">    # possible values are &quot;fixed-delay&quot;, &quot;failure-rate&quot;, &quot;none&quot;, or &quot;fallback&quot; (default)</span><br><span class="line">    type: fallback</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>配置/etc/profile文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/hdp/2.4.0.0-169/hadoop</span><br><span class="line">export YARN_CONF_DIR=/etc/hadoop/conf</span><br><span class="line">export HADOOP_CLASSPATH=`hadoop classpath` #非常重要，不添加 运行flink命令时会报错</span><br></pre></td></tr></table></figure>
<p>在FLink安装目录启动yarn-session.sh:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/yarn-session.sh -n 5 -tm 4096 -s 4 -nm 应用名称 -q 队列名称 -d(这个参数可以保证在我们退出客户端时，任务不被立即杀死，还在yarn上持续运行着)</span><br><span class="line"></span><br><span class="line">yarn-session的参数介绍</span><br><span class="line">  -n ： 指定TaskManager的数量；</span><br><span class="line">  -d: 以分离模式运行；</span><br><span class="line">  -id：指定yarn的任务ID；</span><br><span class="line">  -j:Flink jar文件的路径;</span><br><span class="line">  -jm：JobManager容器的内存（默认值：MB）;</span><br><span class="line">  -nl：为YARN应用程序指定YARN节点标签;</span><br><span class="line">  -nm:在YARN上为应用程序设置自定义名称;</span><br><span class="line">  -q:显示可用的YARN资源（内存，内核）;</span><br><span class="line">  -qu:指定YARN队列;</span><br><span class="line">  -s:指定TaskManager中slot的数量;</span><br><span class="line">  -st:以流模式启动Flink;</span><br><span class="line">  -tm:每个TaskManager容器的内存（默认值：MB）;</span><br><span class="line">  -z:命名空间，用于为高可用性模式创建Zookeeper子路径;</span><br></pre></td></tr></table></figure>
<p>在yarn页面查看Flink-session任务：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eS10ZXN0LTEzMDA2ODkxMDYuY29zLmFwLWNoZW5nZHUubXlxY2xvdWQuY29tLzIwMjAwNDAzMTU0NDMwLnBuZw?x-oss-process=image/format,png"></p>
<p>提交程序报错：</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.client</span><span class="variable">.program</span><span class="variable">.ProgramInvocationException</span>: The main method caused an error: Unable to instantiate java compiler</span><br><span class="line">    ···</span><br><span class="line">Caused by: java<span class="variable">.lang</span><span class="variable">.IllegalStateException</span>: Unable to instantiate java compiler</span><br><span class="line">    ···</span><br><span class="line">Caused by: java<span class="variable">.lang</span><span class="variable">.ClassCastException</span>: org<span class="variable">.codehaus</span><span class="variable">.janino</span><span class="variable">.CompilerFactory</span> cannot be cast to org<span class="variable">.codehaus</span><span class="variable">.commons</span><span class="variable">.compiler</span><span class="variable">.ICompilerFactory</span></span><br></pre></td></tr></table></figure>
<p>解决办法：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.codehaus.janino<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>janino<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.codehaus.janino<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-compiler<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>紧接着又报了 <code>libfb303-0.9.2.jar</code>缺失：</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.client</span><span class="variable">.program</span><span class="variable">.ProgramInvocationException</span>: The main method caused an error: Failed to create Hive Metastore client</span><br><span class="line">Caused by: org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.table</span><span class="variable">.catalog</span><span class="variable">.exceptions</span><span class="variable">.CatalogException</span>: Failed to create Hive Metastore client</span><br><span class="line">Caused by: java<span class="variable">.lang</span><span class="variable">.NoClassDefFoundError</span>: com/facebook/fb303/FacebookService$Iface</span><br><span class="line">Caused by: java<span class="variable">.lang</span><span class="variable">.ClassNotFoundException</span>: com<span class="variable">.facebook</span><span class="variable">.fb303</span><span class="variable">.FacebookService</span>$Iface</span><br></pre></td></tr></table></figure>
<p>将<code>libfb303-0.9.2.jar</code>复制到flink安装路径lib目录下，接着又来一个错误：</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">ava<span class="variable">.lang</span><span class="variable">.LinkageError</span>: ClassCastException: attempting to castjar:file:/data/flink-<span class="number">1</span><span class="variable">.10</span><span class="variable">.0</span>/flink-alter-price-<span class="number">1</span><span class="variable">.0</span>-SNAPSHOT<span class="variable">.jar</span>!/javax/ws/rs/ext/RuntimeDelegate<span class="variable">.classtojar</span>:file:/usr/hdp/<span class="number">2</span><span class="variable">.4</span><span class="variable">.0</span><span class="variable">.0</span>-<span class="number">169</span>/hadoop/lib/jersey-core-<span class="number">1</span><span class="variable">.9</span><span class="variable">.jar</span>!/javax/ws/rs/ext/RuntimeDelegate<span class="variable">.class</span></span><br></pre></td></tr></table></figure>
<p>原因是 <code>jersey-core-1.9.jar</code> 冲突了，解决：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hadoop-compatibility_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.sun.jersey<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jersey-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>以上问题都是把Flink程序的依赖打进jar导致的，直接把依赖jar导进 <code>FLINK_HOME/lib</code>下以上问题基本可以避免。</p>
<p>随即而来是另一个错误：</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">------------------------------------------------------------</span><br><span class="line"> The <span class="keyword">program</span> finished <span class="keyword">with</span> the following exception:</span><br><span class="line"></span><br><span class="line">Caused by: java<span class="variable">.lang</span><span class="variable">.NoClassDefFoundError</span>: org/apache/hadoop/mapred/JobConf</span><br><span class="line"></span><br><span class="line">Caused by: java<span class="variable">.lang</span><span class="variable">.ClassNotFoundException</span>: org<span class="variable">.apache</span><span class="variable">.hadoop</span><span class="variable">.mapred</span><span class="variable">.JobConf</span></span><br><span class="line">	at java<span class="variable">.net</span><span class="variable">.URLClassLoader</span><span class="variable">.findClass</span>(URLClassLoader<span class="variable">.java</span>:<span class="number">381</span>)</span><br><span class="line">	at java<span class="variable">.lang</span><span class="variable">.ClassLoader</span><span class="variable">.loadClass</span>(ClassLoader<span class="variable">.java</span>:<span class="number">424</span>)</span><br><span class="line">	at sun<span class="variable">.misc</span><span class="variable">.Launcher</span>$AppClassLoader<span class="variable">.loadClass</span>(Launcher<span class="variable">.java</span>:<span class="number">331</span>)</span><br><span class="line">	at java<span class="variable">.lang</span><span class="variable">.ClassLoader</span><span class="variable">.loadClass</span>(ClassLoader<span class="variable">.java</span>:<span class="number">357</span>)</span><br><span class="line">	... <span class="number">67</span> more</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个问题足足困扰我三天，痛苦至极，这对于初学者真的不友好，官方文档确实有写，但是很隐蔽，不在Hive集成章节，而是在部署运维里的Hadoop集成里有讲：这里如果官网给的对应Hadoop版本的jar和你的Hadoop正对应就不用下载源码编译了，否则将<code>flink-shaded-hadoop-2-uber</code>源码下载下来，指定对应hadoop版本进行编译，然后打包上传到<code>FLINK_HOME/lib</code>下即可，如果还是不行，看看<code>FLINK_HOME/lib</code>是否缺少<code>flink-hadoop-compatibility_2.11-1.10.0.jar</code>缺少拉进来试试。</p>
<p>注:如果是FLink On Yarn 模式每次修改是最好是杀死session进程，再次重启，方便定位问题。</p>
<p>官方的地址及该问题的说明：</p>
<p><a class="link"   href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/deployment/hadoop.html" >https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/deployment/hadoop.html<i class="fas fa-external-link-alt"></i></a></p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Adding Hadoop to &#x2F;lib</span><br><span class="line">The Flink project releases Hadoop distributions for specific versions, that relocate or exclude several dependencies to reduce the risk of dependency clashes. These can be found in the Additional Components section of the download page. For these versions it is sufficient to download the corresponding Pre-bundled Hadoop component and putting it into the &#x2F;lib directory of the Flink distribution.</span><br><span class="line"></span><br><span class="line">If the used Hadoop version is not listed on the download page (possibly due to being a Vendor-specific version), then it is necessary to build flink-shaded against this version. You can find the source code for this project in the Additional Components section of the download page.</span><br><span class="line"></span><br><span class="line">Note If you want to build flink-shaded against a vendor specific Hadoop version, you first have to configure the vendor-specific maven repository in your local maven setup as described here.</span><br><span class="line"></span><br><span class="line">Run the following command to build and install flink-shaded against your desired Hadoop version (e.g., for version 2.6.5-custom):</span><br><span class="line"></span><br><span class="line">mvn clean install -Dhadoop.version&#x3D;2.6.5-custom</span><br><span class="line">After this step is complete, put the flink-shaded-hadoop-2-uber jar into the &#x2F;lib directory of the Flink distribution.</span><br></pre></td></tr></table></figure>
<p>附上我的<code>FLINK_HOME/lib</code>下jar的截图：</p>
</blockquote>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eS10ZXN0LTEzMDA2ODkxMDYuY29zLmFwLWNoZW5nZHUubXlxY2xvdWQuY29tLzIwMjAwNDA2MTg1OTE3LnBuZw?x-oss-process=image/format,png"></p>
<p>接下来一个小问题的整理(JAR包齐全的话可忽略)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.ClassNotFoundException: org.apache.thrift.TBase</span><br><span class="line">缺少hive-exec的包</span><br></pre></td></tr></table></figure>


<p>启动sql-client：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pcsjob@center1:/data/flink-1.10.0$ bin/sql-client.sh embedded</span><br><span class="line">select * from tmp_flink_test_2 ;</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eS10ZXN0LTEzMDA2ODkxMDYuY29zLmFwLWNoZW5nZHUubXlxY2xvdWQuY29tLzIwMjAwNDA2MTkwMjUyLnBuZw?x-oss-process=image/format,png"></p>
<p>结果正确，大功告成。过程中遇到的问题越多越好，它会培养你解决问题的思路，及时没人指导你也不要放弃，多去官网看看，大部分问题都能解决。</p>
]]></content>
  </entry>
  <entry>
    <title>Window10安装Flink1.10.0</title>
    <url>/2021/01/23/Window10%E5%AE%89%E8%A3%85Flink1-10-0/</url>
    <content><![CDATA[<p>基本可以确定在window10安装Flink1.10.0启动时会出问题，特别恶心，目前无法修复，现在讲一讲我的踩坑经历。</p>
<p>首先我们去官网下载压缩包，官网下载地址：<a class="link"   href="https://flink.apache.org/downloads.html" >https://flink.apache.org/downloads.html<i class="fas fa-external-link-alt"></i></a></p>
<p>一步到位地址：<a class="link"   href="http://archive.apache.org/dist/flink/flink-1.10.0/" >http://archive.apache.org/dist/flink/flink-1.10.0/<i class="fas fa-external-link-alt"></i></a></p>
<p><img src="https://wy-test-1300689106.cos.ap-chengdu.myqcloud.com/format,png-20210123134320533.png"></p>
<p><img src="https://wy-test-1300689106.cos.ap-chengdu.myqcloud.com/format,png-20210123134320533.png"></p>
<p>下载后到本地解压到自己喜欢的一个路径下，尽量不要带中文路径，保持良好的开发规范。</p>
<p>找到对应安装路径下 <code>apache-flink-1.10.0\bin</code>找到 <code>start-cluster.bat</code>双击或者在cmd命令行执行。</p>
<p>正常是会非常顺利的起来，但是如果执行后会默认打开两个cmd窗口，一个是Jobmanager,一个是taskmanager，过了没一会，taskmanager的窗口突然就消失了，访问集群页面 <code>http://localhost:8081/#/overview</code></p>
<p><img src="https://wy-test-1300689106.cos.ap-chengdu.myqcloud.com/format,png-20210123134320533.png"></p>
<p>发现taskmanager没有起来，此时此刻只能去 <code>apache-flink-1.10.0\deps\log</code>路径下去找taskmanager日志文件，查看到底是什么情况，我们会发现有一个 <code>flink-用户名-taskmanager.log</code>文件我们打开文件会提示类似错误</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="number">2020</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">10</span>:<span class="number">06</span>:<span class="number">28</span>,<span class="number">864</span> ERROR org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskManagerRunner</span>       - TaskManager initialization failed.</span><br><span class="line">org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.configuration</span><span class="variable">.IllegalConfigurationException</span>: Failed to create TaskExecutorResourceSpec</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskExecutorResourceUtils</span><span class="variable">.resourceSpecFromConfig</span>(TaskExecutorResourceUtils<span class="variable">.java</span>:<span class="number">72</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskManagerRunner</span><span class="variable">.startTaskManager</span>(TaskManagerRunner<span class="variable">.java</span>:<span class="number">356</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskManagerRunner</span>.&lt;init&gt;(TaskManagerRunner<span class="variable">.java</span>:<span class="number">152</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskManagerRunner</span><span class="variable">.runTaskManager</span>(TaskManagerRunner<span class="variable">.java</span>:<span class="number">308</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskManagerRunner</span><span class="variable">.lambda</span>$runTaskManagerSecurely$<span class="number">2</span>(TaskManagerRunner<span class="variable">.java</span>:<span class="number">322</span>)</span><br><span class="line">	at java<span class="variable">.security</span><span class="variable">.AccessController</span><span class="variable">.doPrivileged</span>(Native Method)</span><br><span class="line">	at javax<span class="variable">.security</span><span class="variable">.auth</span><span class="variable">.Subject</span><span class="variable">.doAs</span>(Subject<span class="variable">.java</span>:<span class="number">422</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.hadoop</span><span class="variable">.security</span><span class="variable">.UserGroupInformation</span><span class="variable">.doAs</span>(UserGroupInformation<span class="variable">.java</span>:<span class="number">1754</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.security</span><span class="variable">.HadoopSecurityContext</span><span class="variable">.runSecured</span>(HadoopSecurityContext<span class="variable">.java</span>:<span class="number">41</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskManagerRunner</span><span class="variable">.runTaskManagerSecurely</span>(TaskManagerRunner<span class="variable">.java</span>:<span class="number">321</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskManagerRunner</span><span class="variable">.main</span>(TaskManagerRunner<span class="variable">.java</span>:<span class="number">287</span>)</span><br><span class="line">Caused by: org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.configuration</span><span class="variable">.IllegalConfigurationException</span>: The required configuration option Key: &#x27;taskmanager<span class="variable">.cpu</span><span class="variable">.cores</span>&#x27; , <span class="keyword">default</span>: <span class="literal">null</span> (fallback keys: []) is <span class="keyword">not</span> set</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskExecutorResourceUtils</span><span class="variable">.checkConfigOptionIsSet</span>(TaskExecutorResourceUtils<span class="variable">.java</span>:<span class="number">90</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskExecutorResourceUtils</span><span class="variable">.lambda</span>$checkTaskExecutorResourceConfigSet$<span class="number">0</span>(TaskExecutorResourceUtils<span class="variable">.java</span>:<span class="number">84</span>)</span><br><span class="line">	at java<span class="variable">.util</span><span class="variable">.Arrays</span>$ArrayList<span class="variable">.forEach</span>(Arrays<span class="variable">.java</span>:<span class="number">3880</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskExecutorResourceUtils</span><span class="variable">.checkTaskExecutorResourceConfigSet</span>(TaskExecutorResourceUtils<span class="variable">.java</span>:<span class="number">84</span>)</span><br><span class="line">	at org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.taskexecutor</span><span class="variable">.TaskExecutorResourceUtils</span><span class="variable">.resourceSpecFromConfig</span>(TaskExecutorResourceUtils<span class="variable">.java</span>:<span class="number">70</span>)</span><br><span class="line">	... <span class="number">10</span> more</span><br><span class="line"><span class="number">2020</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">10</span>:<span class="number">06</span>:<span class="number">28</span>,<span class="number">890</span> INFO  org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.blob</span><span class="variable">.TransientBlobCache</span>              - Shutting down BLOB cache</span><br><span class="line"><span class="number">2020</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">10</span>:<span class="number">06</span>:<span class="number">28</span>,<span class="number">902</span> INFO  org<span class="variable">.apache</span><span class="variable">.flink</span><span class="variable">.runtime</span><span class="variable">.blob</span><span class="variable">.PermanentBlobCache</span>              - Shutting down BLOB cache</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>大致意思是说 <code>taskmanager.cpu.cores</code> 没有设置这个必需具体的参数，默认是NULL，需要我们手动设置一下具体的值，根据机器配置可自行设置参数，接下来重新集群还会提示很多类似的配置参数没设置的错误，而且每次只会提示一条，笔者在这里把所有的坑都踩了一下，最后的问题基本是window系统与Flink1.10.0不太兼容，建议换成Linux或者MAC环境。</p>
]]></content>
  </entry>
  <entry>
    <title>FLink读取+插入Hive数据入坑指南</title>
    <url>/2021/01/23/FLink%E8%AF%BB%E5%8F%96-%E6%8F%92%E5%85%A5Hive%E6%95%B0%E6%8D%AE%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<p>Flink<code>1.9</code>以上版本可以使用hivecatalog读取Hive数据，但是<code>1.9</code>对于Hive的版本支持不太友好，只支持<code>2.3.4</code>和<code>1.2.1</code>，笔者用的Hive版本是比较老的版本1.2.1,FLink是<code>1.10.0</code>,接下来说一说我在读取Hive数据和插入Hive数据期间遇到的问题。</p>
<p>本地环境：window10，Flink：1.10.0 </p>
<p>目的：用本地电脑IDEA运行Flink程序读取测试环境集群的Hive表数据</p>
<p>首先我们可以参照Flink的官方文档加入任务需要的依赖：官网地址：<a class="link"   href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/hive/" >https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/hive/<i class="fas fa-external-link-alt"></i></a></p>
<p>Flink中文社区的FLInk-Hive文章：<a class="link"   href="https://ververica.cn/developers/flink1-9-hive/" >https://ververica.cn/developers/flink1-9-hive/<i class="fas fa-external-link-alt"></i></a></p>
<p>如图是官网提供的需要的依赖：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!-- Flink Dependency --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-connector-hive_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-api-java-bridge_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Hive Dependency --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<p>然后在主程序里写好代码，运行是发现报了一个错：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.lang.IllegalArgumentException: java.net.UnknownHostException: tesla-cluster</span><br></pre></td></tr></table></figure>
<p>这里的意思是说找不到hdfs的地址，需要我们手动把测试环境的hdfs-site.xml复制到本地，放到resources目录下，此问题解决。</p>
<p>再次运行代码，发现又报一个错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.facebook.fb303.FacebookService$Client.sendBaseOneway(Ljava&#x2F;lang&#x2F;String;Lorg&#x2F;apache&#x2F;thrift&#x2F;TBase;)V</span><br></pre></td></tr></table></figure>
<p>在网上找了很多资料后来发现是Jar包冲突，原因是我从Flink1.9.1迁移过来的，FLink1.9.1提供的依赖是这样的：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hadoop Dependencies --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hadoop-compatibility_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive 1.2.1 is built with Hadoop 2.6.0. We pick 2.6.5 which flink-shaded-hadoop is pre-built with, but users can pick their own hadoop version, as long as it&#x27;s compatible with Hadoop 2.6.0 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-shaded-hadoop-2-uber<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.5-8.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive Metastore --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-metastore<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.thrift<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>libfb303<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.9.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>比Flink1.10多出来几个Jar包，其中的罪魁祸首是：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.thrift<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>libfb303<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.9.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>它于hive-exec包里的版本冲突了，将这个依赖删除就好了。你的程序可能会因为别的依赖导致冲突，找到对应的冲突的jar删除一个不需要的版本，就好了。</p>
<p>最后附上最基础的Flink读取Hive数据代码：</p>
<p>主程序：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.catalog.hive.HiveCatalog;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>:wy</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span>:2020/3/31</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span>:1.0.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IntelligenceAlter</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();</span><br><span class="line">        TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line">        String name            = <span class="string">&quot;hive&quot;</span>;</span><br><span class="line">        String defaultDatabase = <span class="string">&quot;tmp&quot;</span>;</span><br><span class="line">        String hiveConfDir     = <span class="string">&quot;E:\\Hadoop\\&quot;</span>;</span><br><span class="line">        String version         = <span class="string">&quot;1.2.1&quot;</span>; </span><br><span class="line"></span><br><span class="line">        HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir, version);</span><br><span class="line">        tableEnv.registerCatalog(<span class="string">&quot;hive&quot;</span>, hive);</span><br><span class="line">        tableEnv.registerCatalog(name, hive);</span><br><span class="line">        tableEnv.useCatalog(name);</span><br><span class="line"></span><br><span class="line">        tableEnv.sqlQuery(<span class="string">&quot;select * from tmp.tmp_flink_test_2&quot;</span>).select(<span class="string">&quot;product_id&quot;</span>);</span><br><span class="line">        tableEnv.sqlUpdate(<span class="string">&quot;insert into tmp.tmp_flink_test_2 values (&#x27;newKey&#x27;)&quot;</span>);</span><br><span class="line">        tableEnv.execute(<span class="string">&quot;insert into tmp&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>pom.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hadoop Dependencies --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hadoop-compatibility_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive 1.2.1 is built with Hadoop 2.6.0. We pick 2.6.5 which flink-shaded-hadoop is pre-built with, but users can pick their own hadoop version, as long as it&#x27;s compatible with Hadoop 2.6.0 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-shaded-hadoop-2-uber<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.5-8.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>这次试手Flink从kafka读数据写入hbase，遇到了很大的坑</title>
    <url>/2021/01/23/%E8%BF%99%E6%AC%A1%E8%AF%95%E6%89%8BFlink%E4%BB%8Ekafka%E8%AF%BB%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5hbase%EF%BC%8C%E9%81%87%E5%88%B0%E4%BA%86%E5%BE%88%E5%A4%A7%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<p>1.我的程序是用Flink 1.9.1从本地kafka读取数据，写到本地hbase，本地zookeeper和kafka服务都起好了，开始运行程序，没有报错信息，就是一直读不到kafka的数据，在kafka生产者命令窗口都输入10条了，我想怎么还没开始读数据，我也没设置时间窗口啊，见鬼了</p>
<p>答：这种问题99%都是因为你的kafka连接依赖版本不对，如果你现在是1.1不妨改成0.9试试，或许可以读出来了，相反也可以试试。</p>
<p>注：别忘了在flink代码addsource时也要用“FlinkKafkaConsumer09”，不过你改完依赖不改这个，IDEA会提示你的，没多大事</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>2.程序运行起来没问题，kafka也读出数据了，但是一直卡在连接hbase步骤，不失败也不报错，这个开始以为是hbase-client引用版本的事情，特意去maven官网去查了查对应支持的版本，发现没问题啊，为啥这样对我呢？</p>
<p>答：这个问题99%是因为没有找到zookeeper的主机，程序在不停的尝试连接你配置的主机，就是连不上，你说气人不?但是像我这种人没有服务器的主，连接的是本地的地址啊“127.0.0.1”，为啥还会这样呢，讲不讲理？本地也找不到了？？？？这个时候看看你有没有连接什么代理工具，你要是老老实实连个WiFi不至于这样，把代理关了，再试试，或许真的连上了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;127.0.0.1&quot;);</span><br></pre></td></tr></table></figure>

<p>3.还有一种情况实在本地运行不易发生的，但是我必须说，线上很容易出问题，此时将写入hbase的配置信息的zookeeper连接地址改为服务器的地址，然后运行程序，这个时候读取kafka一点问题没有，写入hbase报空指针，死活写不进去，你说咋办吧，网上有很多博客说这个事，但是很多都不解决问题或者不适合我们的问题。</p>
<p>答：这个可能是我们程序找不到hbase在zookeeper的目录了，跟默认的不一致，我们最好去zk客户端里边找找我们的hbase的目录之后再填写这个参数，保险些。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">configuration.set(&quot;zookeeper.znode.parent&quot;,&quot;&#x2F;hbase-unsecure&quot;);</span><br></pre></td></tr></table></figure>
<p>最后附上我的垃圾代码，仅供参考，你要运行起来之后可能会发现Hbase之插进一条记录，那是我的row_key、列族和列名都写死了，导致不断的覆盖value，你可以给row_key一个变量，最常见的当前时间戳。</p>
<p>pom.xml:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;com.wy&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink2hbase&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hbase-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.1.2&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.phoenix&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;phoenix-core&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;4.14.1-HBase-1.1&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-streaming-java_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.projectlombok&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;lombok&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.18.4&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure>
<p>主程序：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.hbase.client.*;</span><br><span class="line"></span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class flinkhbase &#123;</span><br><span class="line">    public static Configuration configuration;</span><br><span class="line">    public static Connection connection;</span><br><span class="line">    public static Admin admin;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(1);</span><br><span class="line">        Properties properties &#x3D; new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line">        FlinkKafkaConsumer09&lt;String&gt; consumer &#x3D; new FlinkKafkaConsumer09&lt;String&gt;(&quot;sinkTest&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        &#x2F;&#x2F;从最早开始消费</span><br><span class="line">        consumer.setStartFromEarliest();</span><br><span class="line">        DataStream&lt;String&gt; stream &#x3D; env.addSource(consumer);</span><br><span class="line">        stream.print();</span><br><span class="line">        stream.process(new HbaseProcess());</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>写入Hbase：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import lombok.extern.slf4j.Slf4j;</span><br><span class="line">import org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line">import org.apache.flink.util.Collector;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.TableName;</span><br><span class="line">import org.apache.hadoop.hbase.client.*;</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line">@Slf4j</span><br><span class="line">public class HbaseProcess extends ProcessFunction&lt;String, String&gt; &#123;</span><br><span class="line">    private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line"></span><br><span class="line">    private Connection connection &#x3D; null;</span><br><span class="line">    private Table table &#x3D; null;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void open(org.apache.flink.configuration.Configuration parameters) throws Exception &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            &#x2F;&#x2F; 加载HBase的配置</span><br><span class="line">            Configuration configuration &#x3D; HBaseConfiguration.create();</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 读取配置文件</span><br><span class="line">            configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;127.0.0.1&quot;);</span><br><span class="line">            configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</span><br><span class="line">            configuration.setInt(&quot;hbase.rpc.timeout&quot;, 30000);</span><br><span class="line">            configuration.setInt(&quot;hbase.client.operation.timeout&quot;, 30000);</span><br><span class="line">            configuration.setInt(&quot;hbase.client.scanner.timeout.period&quot;, 30000);</span><br><span class="line">&#x2F;&#x2F;            configuration.set(&quot;zookeeper.znode.parent&quot;,&quot;&#x2F;hbase-unsecure&quot;);</span><br><span class="line">            configuration.set(&quot;hbase.master&quot;,&quot;localhost:60010&quot;);</span><br><span class="line">            connection &#x3D; ConnectionFactory.createConnection(configuration);</span><br><span class="line"></span><br><span class="line">            HBaseAdmin hbaseadmin &#x3D; new HBaseAdmin(connection);</span><br><span class="line"></span><br><span class="line">            TableName tableName &#x3D; TableName.valueOf(&quot;ygc_test&quot;);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 获取表对象</span><br><span class="line">            table &#x3D; connection.getTable(tableName);</span><br><span class="line"></span><br><span class="line">            System.out.println(hbaseadmin.tableExists(tableName));</span><br><span class="line"></span><br><span class="line">            System.out.println(&quot;[HbaseSink] : open HbaseSink finished&quot;);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            System.out.println(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close() throws Exception &#123;</span><br><span class="line">        System.out.println(&quot;close...&quot;);</span><br><span class="line">        if (null !&#x3D; table) table.close();</span><br><span class="line">        if (null !&#x3D; connection) connection.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void processElement(String value, Context ctx, Collector&lt;String&gt; out) throws Exception &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            System.out.println(&quot;输入的值:&quot;+value);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;row1:cf:a:aaa</span><br><span class="line">            String[] split &#x3D; value.split(&quot;:&quot;);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 创建一个put请求，用于添加数据或者更新数据</span><br><span class="line">            Put put &#x3D; new Put(Bytes.toBytes(&quot;1002&quot;));</span><br><span class="line">            put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;a&quot;), Bytes.toBytes(value));</span><br><span class="line">            table.put(put);</span><br><span class="line">            System.out.println(&quot;插入成功&quot;);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            System.out.println(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>元数据</title>
    <url>/2021/01/23/%E5%85%83%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h3 id="1-元数据的定义："><a href="#1-元数据的定义：" class="headerlink" title="1.元数据的定义："></a>1.元数据的定义：</h3><blockquote>
<p>元数据是关于数据的数据，元数据打通了源数据、数据仓库、数据应用、记录了数据从产生到消费的全过程。元数据主要记录数据仓库中魔性的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。在数据仓库系统中，元数据可以帮助数据仓库管理员和开发人员非常方便地找到他们所关心的数据，用于指导其进行数据管理和开发工作，提高工作效率。  –《大数据之路》</p>
</blockquote>
<p>元数据又分为技术元数据和业务元数据两大类：</p>
<ul>
<li>技术元数据：Hive的表结构信息分区信息等，MapReduce任务执行信息，oozie，宙斯，阿兹卡班任务的运行信息，依赖信息</li>
<li>业务元数据：数据报表的口径定义，指标清洗规则等，用来解释某些业务流程或者指标的由来</li>
</ul>
<h3 id="2-元数据价值"><a href="#2-元数据价值" class="headerlink" title="2.元数据价值"></a>2.元数据价值</h3><blockquote>
<p>元数据最重要的应用价值是数据管理、数据内容、数据应用的基础，在数据管理方面为集团数据提供在计算、存储、成本、质量、安全、模型等治理领域上的数据支持。  –《大数据之路》</p>
</blockquote>
<p>元数据对应用链路分析和数据建模还有驱动ETL开发方向非常重要。</p>
]]></content>
  </entry>
</search>
